{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concise Implementation of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The surge of deep learning has inspired the development of a variety of mature software frameworks, that\n",
    "automate much of the repetitive work of implementing deep learning models. In the previous section we\n",
    "relied only on NDarray for data storage and linear algebra and the auto-differentiation capabilities in the\n",
    "autograd package. In practice, because many of the more abstract operations, e.g. data iterators, loss\n",
    "functions, model architectures, and optimizers, are so common, deep learning libraries will give us library\n",
    "functions for these as well.\n",
    "\n",
    "We have used DataLoader to load the MNIST dataset in Section 4.5. In this section, we will learn how we can\n",
    "implement the linear regression model in Section 5.2 much more concisely with DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generating Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we will generate the same data set as that used in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T15:40:47.871841Z",
     "start_time": "2025-02-17T15:40:47.868624Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    \"\"\"generate y = X w + b + noise\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, w.shape[0]))\n",
    "    y = X @ w + b\n",
    "    y += torch.normal(0, 0.01, size=y.shape)\n",
    "    assert X.shape[0] == y.shape[0], f\"The shape of X: {X.shape[0]} mismatched the shape of y: {y.shape[0]}\"\n",
    "    X.type(torch.FloatTensor)\n",
    "    y.type(torch.FloatTensor)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "true_w = torch.Tensor([2, -3.4])\n",
    "true_w = true_w.view(-1, 1)\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than rolling our own iterator, we can call upon DataLoader module to read data. The first step will be to instantiate an ArrayDataset, which takes in one or more NDArrays as arguments. Here, we pass in features and\n",
    "labels as arguments. Next, we will use the ArrayDataset to instantiate a DataLoader, which also requires\n",
    "that we specify a batch_size and specify a Boolean value shuffle indicating whether or not we want the\n",
    "DataLoader to shuffle the data on each epoch (pass through the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T15:40:56.729988Z",
     "start_time": "2025-02-17T15:40:56.727072Z"
    }
   },
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = TensorDataset(*(features, labels))\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use data_iter in much the same way as we called the data_iter function in the previous\n",
    "section. To verify that it’s working, we can read and print the first mini-batch of instances."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T15:41:26.779858Z",
     "start_time": "2025-02-17T15:41:26.771551Z"
    }
   },
   "source": [
    "for X, y in data_iter:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0337, -0.5465],\n",
      "        [ 0.4571, -0.1845],\n",
      "        [ 0.7293, -0.5190],\n",
      "        [ 0.1659, -0.7009],\n",
      "        [ 1.2442, -0.4710],\n",
      "        [ 0.3775,  0.8185],\n",
      "        [-0.4285, -0.9860],\n",
      "        [-0.2462,  2.2888],\n",
      "        [ 0.9067,  0.8895],\n",
      "        [-0.3329,  0.0447]])\n",
      "tensor([[ 6.1263],\n",
      "        [ 5.7342],\n",
      "        [ 7.4221],\n",
      "        [ 6.9076],\n",
      "        [ 8.2892],\n",
      "        [ 2.1670],\n",
      "        [ 6.7201],\n",
      "        [-4.0906],\n",
      "        [ 2.9703],\n",
      "        [ 3.3876]])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we implemented linear regression from scratch in the previous section, we had to define the model\n",
    "parameters and explicitly write out the calculation to produce output using basic linear algebra operations.\n",
    "You should know how to do this. But once your models get more complex, even qualitatively simple changes\n",
    "to the model might result in many low-level changes.\n",
    "\n",
    "We import torch.nn as nn .For standard operations, we can use nn's predefined layers, which allow us to focus especially on thelayers used to construct the model rather than having to focus on the implementation.\n",
    "To define a linear model, we first import the nn module, which defines a large number of neural network\n",
    "layers (note that “nn” is an abbreviation for neural networks). We will first define a model variable net,\n",
    "which is a Sequential instance. In nn, a Sequential instance can be regarded as a container that\n",
    "concatenates the various layers in sequence. When input data is given, each layer in the container will be\n",
    "calculated in order, and the output of one layer will be the input of the next layer. In this example, since\n",
    "our model consists of only one layer, we do not really need Sequential. But since nearly all of our future\n",
    "models will involve multiple layers, let’s get into the habit early.\n",
    "Recall the architecture of a single layer network. The layer is fully connected since it connects all inputs\n",
    "with all outputs by means of a matrix-vector multiplication. In nn, the fully-connected layer is defined\n",
    "in the Linear class. Since we only want to generate a single scalar output,we set that number to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T15:46:10.808597Z",
     "start_time": "2025-02-17T15:46:10.799571Z"
    }
   },
   "source": [
    "from IPython.display import SVG\n",
    "\n",
    "SVG(filename=\"../img/singleneuron.svg\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ],
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"263pt\" height=\"88pt\" viewBox=\"0 0 263 88\" version=\"1.1\">\n<defs>\n<g>\n<symbol overflow=\"visible\" id=\"glyph0-0\">\n<path style=\"stroke:none;\" d=\"M 1.125 0 L 1.125 -5.625 L 5.625 -5.625 L 5.625 0 Z M 1.265625 -0.140625 L 5.484375 -0.140625 L 5.484375 -5.484375 L 1.265625 -5.484375 Z M 1.265625 -0.140625 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-1\">\n<path style=\"stroke:none;\" d=\"M -0.015625 0 L 2.015625 -2.375 L 0.859375 -4.671875 L 1.734375 -4.671875 L 2.125 -3.84375 C 2.269531 -3.53125 2.398438 -3.226562 2.515625 -2.9375 L 3.875 -4.671875 L 4.84375 -4.671875 L 2.875 -2.3125 L 4.0625 0 L 3.171875 0 L 2.71875 -0.953125 C 2.613281 -1.148438 2.5 -1.398438 2.375 -1.703125 L 0.984375 0 Z M -0.015625 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph0-2\">\n<path style=\"stroke:none;\" d=\"M 0.4375 -1.765625 C 0.4375 -2.679688 0.707031 -3.4375 1.25 -4.03125 C 1.6875 -4.519531 2.265625 -4.765625 2.984375 -4.765625 C 3.546875 -4.765625 4 -4.585938 4.34375 -4.234375 C 4.6875 -3.890625 4.859375 -3.421875 4.859375 -2.828125 C 4.859375 -2.285156 4.75 -1.785156 4.53125 -1.328125 C 4.3125 -0.867188 4.003906 -0.515625 3.609375 -0.265625 C 3.210938 -0.015625 2.789062 0.109375 2.34375 0.109375 C 1.976562 0.109375 1.644531 0.03125 1.34375 -0.125 C 1.050781 -0.28125 0.828125 -0.5 0.671875 -0.78125 C 0.515625 -1.070312 0.4375 -1.398438 0.4375 -1.765625 Z M 1.234375 -1.84375 C 1.234375 -1.40625 1.335938 -1.070312 1.546875 -0.84375 C 1.765625 -0.625 2.035156 -0.515625 2.359375 -0.515625 C 2.523438 -0.515625 2.691406 -0.546875 2.859375 -0.609375 C 3.023438 -0.679688 3.179688 -0.785156 3.328125 -0.921875 C 3.472656 -1.066406 3.59375 -1.226562 3.6875 -1.40625 C 3.789062 -1.582031 3.875 -1.773438 3.9375 -1.984375 C 4.03125 -2.273438 4.078125 -2.554688 4.078125 -2.828125 C 4.078125 -3.242188 3.96875 -3.566406 3.75 -3.796875 C 3.539062 -4.035156 3.273438 -4.15625 2.953125 -4.15625 C 2.703125 -4.15625 2.472656 -4.09375 2.265625 -3.96875 C 2.066406 -3.851562 1.882812 -3.679688 1.71875 -3.453125 C 1.550781 -3.222656 1.425781 -2.957031 1.34375 -2.65625 C 1.269531 -2.351562 1.234375 -2.082031 1.234375 -1.84375 Z M 1.234375 -1.84375 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph1-0\">\n<path style=\"stroke:none;\" d=\"M 0.875 0 L 0.875 -4.375 L 4.375 -4.375 L 4.375 0 Z M 0.984375 -0.109375 L 4.265625 -0.109375 L 4.265625 -4.265625 L 0.984375 -4.265625 Z M 0.984375 -0.109375 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph1-1\">\n<path style=\"stroke:none;\" d=\"M 1.6875 0 L 2.484375 -3.78125 C 2.140625 -3.507812 1.65625 -3.296875 1.03125 -3.140625 L 1.15625 -3.703125 C 1.457031 -3.828125 1.757812 -3.988281 2.0625 -4.1875 C 2.363281 -4.382812 2.585938 -4.554688 2.734375 -4.703125 C 2.828125 -4.796875 2.914062 -4.90625 3 -5.03125 L 3.359375 -5.03125 L 2.3125 0 Z M 1.6875 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph1-2\">\n<path style=\"stroke:none;\" d=\"M 0.40625 0 C 0.46875 -0.300781 0.554688 -0.550781 0.671875 -0.75 C 0.785156 -0.945312 0.9375 -1.132812 1.125 -1.3125 C 1.3125 -1.5 1.671875 -1.804688 2.203125 -2.234375 C 2.523438 -2.492188 2.75 -2.6875 2.875 -2.8125 C 3.039062 -2.988281 3.160156 -3.160156 3.234375 -3.328125 C 3.285156 -3.441406 3.3125 -3.566406 3.3125 -3.703125 C 3.3125 -3.929688 3.226562 -4.125 3.0625 -4.28125 C 2.90625 -4.445312 2.707031 -4.53125 2.46875 -4.53125 C 2.238281 -4.53125 2.035156 -4.445312 1.859375 -4.28125 C 1.679688 -4.125 1.554688 -3.863281 1.484375 -3.5 L 0.875 -3.59375 C 0.9375 -4.039062 1.109375 -4.390625 1.390625 -4.640625 C 1.679688 -4.898438 2.039062 -5.03125 2.46875 -5.03125 C 2.75 -5.03125 3.003906 -4.96875 3.234375 -4.84375 C 3.460938 -4.726562 3.632812 -4.5625 3.75 -4.34375 C 3.875 -4.132812 3.9375 -3.914062 3.9375 -3.6875 C 3.9375 -3.351562 3.816406 -3.035156 3.578125 -2.734375 C 3.429688 -2.535156 3.003906 -2.15625 2.296875 -1.59375 C 1.984375 -1.351562 1.753906 -1.15625 1.609375 -1 C 1.460938 -0.84375 1.351562 -0.695312 1.28125 -0.5625 L 3.515625 -0.5625 L 3.390625 0 Z M 0.40625 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph1-3\">\n<path style=\"stroke:none;\" d=\"M 2.6875 -0.53125 C 2.332031 -0.125 1.960938 0.078125 1.578125 0.078125 C 1.234375 0.078125 0.945312 -0.046875 0.71875 -0.296875 C 0.488281 -0.554688 0.375 -0.925781 0.375 -1.40625 C 0.375 -1.84375 0.460938 -2.242188 0.640625 -2.609375 C 0.816406 -2.984375 1.039062 -3.257812 1.3125 -3.4375 C 1.59375 -3.625 1.867188 -3.71875 2.140625 -3.71875 C 2.585938 -3.71875 2.925781 -3.5 3.15625 -3.0625 L 3.578125 -5.015625 L 4.1875 -5.015625 L 3.140625 0 L 2.578125 0 Z M 0.984375 -1.515625 C 0.984375 -1.265625 1.007812 -1.066406 1.0625 -0.921875 C 1.113281 -0.773438 1.195312 -0.648438 1.3125 -0.546875 C 1.4375 -0.453125 1.582031 -0.40625 1.75 -0.40625 C 2.03125 -0.40625 2.285156 -0.550781 2.515625 -0.84375 C 2.816406 -1.238281 2.96875 -1.71875 2.96875 -2.28125 C 2.96875 -2.570312 2.890625 -2.796875 2.734375 -2.953125 C 2.585938 -3.117188 2.398438 -3.203125 2.171875 -3.203125 C 2.023438 -3.203125 1.890625 -3.164062 1.765625 -3.09375 C 1.648438 -3.03125 1.53125 -2.921875 1.40625 -2.765625 C 1.289062 -2.609375 1.191406 -2.40625 1.109375 -2.15625 C 1.023438 -1.914062 0.984375 -1.703125 0.984375 -1.515625 Z M 0.984375 -1.515625 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-0\">\n<path style=\"stroke:none;\" d=\"M 1.125 0 L 1.125 -5.625 L 5.625 -5.625 L 5.625 0 Z M 1.265625 -0.140625 L 5.484375 -0.140625 L 5.484375 -5.484375 L 1.265625 -5.484375 Z M 1.265625 -0.140625 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-1\">\n<path style=\"stroke:none;\" d=\"M 0.84375 0 L 0.84375 -6.4375 L 1.6875 -6.4375 L 1.6875 0 Z M 0.84375 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-2\">\n<path style=\"stroke:none;\" d=\"M 0.59375 0 L 0.59375 -4.671875 L 1.3125 -4.671875 L 1.3125 -4 C 1.644531 -4.507812 2.140625 -4.765625 2.796875 -4.765625 C 3.078125 -4.765625 3.332031 -4.710938 3.5625 -4.609375 C 3.800781 -4.515625 3.976562 -4.382812 4.09375 -4.21875 C 4.207031 -4.0625 4.289062 -3.867188 4.34375 -3.640625 C 4.375 -3.492188 4.390625 -3.238281 4.390625 -2.875 L 4.390625 0 L 3.59375 0 L 3.59375 -2.84375 C 3.59375 -3.164062 3.5625 -3.40625 3.5 -3.5625 C 3.4375 -3.71875 3.328125 -3.84375 3.171875 -3.9375 C 3.015625 -4.039062 2.832031 -4.09375 2.625 -4.09375 C 2.289062 -4.09375 2 -3.984375 1.75 -3.765625 C 1.507812 -3.554688 1.390625 -3.148438 1.390625 -2.546875 L 1.390625 0 Z M 0.59375 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-3\">\n<path style=\"stroke:none;\" d=\"M 0.59375 1.78125 L 0.59375 -4.671875 L 1.3125 -4.671875 L 1.3125 -4.0625 C 1.476562 -4.300781 1.664062 -4.476562 1.875 -4.59375 C 2.09375 -4.707031 2.359375 -4.765625 2.671875 -4.765625 C 3.066406 -4.765625 3.414062 -4.660156 3.71875 -4.453125 C 4.019531 -4.253906 4.25 -3.96875 4.40625 -3.59375 C 4.5625 -3.21875 4.640625 -2.8125 4.640625 -2.375 C 4.640625 -1.894531 4.550781 -1.460938 4.375 -1.078125 C 4.207031 -0.691406 3.960938 -0.394531 3.640625 -0.1875 C 3.316406 0.0078125 2.972656 0.109375 2.609375 0.109375 C 2.347656 0.109375 2.113281 0.0507812 1.90625 -0.0625 C 1.695312 -0.175781 1.523438 -0.316406 1.390625 -0.484375 L 1.390625 1.78125 Z M 1.3125 -2.3125 C 1.3125 -1.707031 1.429688 -1.257812 1.671875 -0.96875 C 1.921875 -0.6875 2.21875 -0.546875 2.5625 -0.546875 C 2.90625 -0.546875 3.203125 -0.691406 3.453125 -0.984375 C 3.710938 -1.285156 3.84375 -1.75 3.84375 -2.375 C 3.84375 -2.96875 3.71875 -3.410156 3.46875 -3.703125 C 3.226562 -4.003906 2.9375 -4.15625 2.59375 -4.15625 C 2.257812 -4.15625 1.960938 -3.992188 1.703125 -3.671875 C 1.441406 -3.359375 1.3125 -2.90625 1.3125 -2.3125 Z M 1.3125 -2.3125 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-4\">\n<path style=\"stroke:none;\" d=\"M 3.65625 0 L 3.65625 -0.6875 C 3.289062 -0.15625 2.796875 0.109375 2.171875 0.109375 C 1.898438 0.109375 1.644531 0.0546875 1.40625 -0.046875 C 1.164062 -0.160156 0.984375 -0.296875 0.859375 -0.453125 C 0.742188 -0.609375 0.664062 -0.800781 0.625 -1.03125 C 0.59375 -1.1875 0.578125 -1.4375 0.578125 -1.78125 L 0.578125 -4.671875 L 1.359375 -4.671875 L 1.359375 -2.078125 C 1.359375 -1.660156 1.378906 -1.382812 1.421875 -1.25 C 1.460938 -1.039062 1.5625 -0.875 1.71875 -0.75 C 1.882812 -0.632812 2.085938 -0.578125 2.328125 -0.578125 C 2.566406 -0.578125 2.789062 -0.632812 3 -0.75 C 3.207031 -0.875 3.351562 -1.039062 3.4375 -1.25 C 3.519531 -1.457031 3.5625 -1.765625 3.5625 -2.171875 L 3.5625 -4.671875 L 4.359375 -4.671875 L 4.359375 0 Z M 3.65625 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-5\">\n<path style=\"stroke:none;\" d=\"M 2.328125 -0.703125 L 2.4375 -0.015625 C 2.207031 0.0351562 2.007812 0.0625 1.84375 0.0625 C 1.550781 0.0625 1.328125 0.015625 1.171875 -0.078125 C 1.015625 -0.171875 0.898438 -0.289062 0.828125 -0.4375 C 0.765625 -0.582031 0.734375 -0.890625 0.734375 -1.359375 L 0.734375 -4.046875 L 0.15625 -4.046875 L 0.15625 -4.671875 L 0.734375 -4.671875 L 0.734375 -5.828125 L 1.53125 -6.296875 L 1.53125 -4.671875 L 2.328125 -4.671875 L 2.328125 -4.046875 L 1.53125 -4.046875 L 1.53125 -1.328125 C 1.53125 -1.097656 1.539062 -0.953125 1.5625 -0.890625 C 1.59375 -0.828125 1.640625 -0.773438 1.703125 -0.734375 C 1.765625 -0.691406 1.851562 -0.671875 1.96875 -0.671875 C 2.0625 -0.671875 2.179688 -0.679688 2.328125 -0.703125 Z M 2.328125 -0.703125 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-6\">\n<path style=\"stroke:none;\" d=\"\"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-7\">\n<path style=\"stroke:none;\" d=\"M 0.578125 0 L 0.578125 -6.4375 L 1.359375 -6.4375 L 1.359375 0 Z M 0.578125 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-8\">\n<path style=\"stroke:none;\" d=\"M 3.640625 -0.578125 C 3.347656 -0.328125 3.066406 -0.148438 2.796875 -0.046875 C 2.523438 0.0546875 2.234375 0.109375 1.921875 0.109375 C 1.410156 0.109375 1.015625 -0.015625 0.734375 -0.265625 C 0.460938 -0.515625 0.328125 -0.835938 0.328125 -1.234375 C 0.328125 -1.460938 0.378906 -1.671875 0.484375 -1.859375 C 0.585938 -2.046875 0.722656 -2.195312 0.890625 -2.3125 C 1.054688 -2.425781 1.242188 -2.515625 1.453125 -2.578125 C 1.609375 -2.609375 1.84375 -2.644531 2.15625 -2.6875 C 2.800781 -2.757812 3.273438 -2.851562 3.578125 -2.96875 C 3.578125 -3.070312 3.578125 -3.140625 3.578125 -3.171875 C 3.578125 -3.492188 3.503906 -3.71875 3.359375 -3.84375 C 3.148438 -4.03125 2.847656 -4.125 2.453125 -4.125 C 2.078125 -4.125 1.800781 -4.054688 1.625 -3.921875 C 1.445312 -3.796875 1.316406 -3.566406 1.234375 -3.234375 L 0.46875 -3.328125 C 0.53125 -3.660156 0.640625 -3.925781 0.796875 -4.125 C 0.960938 -4.332031 1.195312 -4.488281 1.5 -4.59375 C 1.8125 -4.707031 2.164062 -4.765625 2.5625 -4.765625 C 2.96875 -4.765625 3.289062 -4.71875 3.53125 -4.625 C 3.78125 -4.53125 3.960938 -4.410156 4.078125 -4.265625 C 4.203125 -4.128906 4.285156 -3.953125 4.328125 -3.734375 C 4.359375 -3.597656 4.375 -3.359375 4.375 -3.015625 L 4.375 -1.953125 C 4.375 -1.222656 4.390625 -0.757812 4.421875 -0.5625 C 4.453125 -0.363281 4.519531 -0.175781 4.625 0 L 3.796875 0 C 3.710938 -0.164062 3.660156 -0.359375 3.640625 -0.578125 Z M 3.578125 -2.34375 C 3.285156 -2.226562 2.851562 -2.128906 2.28125 -2.046875 C 1.957031 -1.992188 1.726562 -1.9375 1.59375 -1.875 C 1.457031 -1.820312 1.351562 -1.738281 1.28125 -1.625 C 1.207031 -1.507812 1.171875 -1.382812 1.171875 -1.25 C 1.171875 -1.039062 1.25 -0.863281 1.40625 -0.71875 C 1.5625 -0.582031 1.796875 -0.515625 2.109375 -0.515625 C 2.410156 -0.515625 2.679688 -0.582031 2.921875 -0.71875 C 3.160156 -0.851562 3.335938 -1.035156 3.453125 -1.265625 C 3.535156 -1.441406 3.578125 -1.703125 3.578125 -2.046875 Z M 3.578125 -2.34375 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-9\">\n<path style=\"stroke:none;\" d=\"M 0.5625 1.796875 L 0.46875 1.0625 C 0.644531 1.101562 0.796875 1.125 0.921875 1.125 C 1.097656 1.125 1.238281 1.09375 1.34375 1.03125 C 1.445312 0.976562 1.535156 0.898438 1.609375 0.796875 C 1.648438 0.710938 1.726562 0.515625 1.84375 0.203125 C 1.863281 0.160156 1.890625 0.0976562 1.921875 0.015625 L 0.140625 -4.671875 L 1 -4.671875 L 1.96875 -1.96875 C 2.09375 -1.625 2.207031 -1.265625 2.3125 -0.890625 C 2.394531 -1.242188 2.5 -1.597656 2.625 -1.953125 L 3.625 -4.671875 L 4.421875 -4.671875 L 2.640625 0.078125 C 2.453125 0.585938 2.304688 0.941406 2.203125 1.140625 C 2.054688 1.398438 1.894531 1.585938 1.71875 1.703125 C 1.539062 1.828125 1.320312 1.890625 1.0625 1.890625 C 0.914062 1.890625 0.75 1.859375 0.5625 1.796875 Z M 0.5625 1.796875 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-10\">\n<path style=\"stroke:none;\" d=\"M 3.78125 -1.5 L 4.609375 -1.40625 C 4.472656 -0.925781 4.226562 -0.550781 3.875 -0.28125 C 3.53125 -0.0195312 3.085938 0.109375 2.546875 0.109375 C 1.867188 0.109375 1.328125 -0.0976562 0.921875 -0.515625 C 0.523438 -0.941406 0.328125 -1.535156 0.328125 -2.296875 C 0.328125 -3.078125 0.53125 -3.679688 0.9375 -4.109375 C 1.34375 -4.546875 1.867188 -4.765625 2.515625 -4.765625 C 3.140625 -4.765625 3.644531 -4.550781 4.03125 -4.125 C 4.425781 -3.707031 4.625 -3.113281 4.625 -2.34375 C 4.625 -2.289062 4.625 -2.21875 4.625 -2.125 L 1.140625 -2.125 C 1.171875 -1.613281 1.316406 -1.222656 1.578125 -0.953125 C 1.835938 -0.679688 2.164062 -0.546875 2.5625 -0.546875 C 2.851562 -0.546875 3.097656 -0.617188 3.296875 -0.765625 C 3.503906 -0.921875 3.664062 -1.164062 3.78125 -1.5 Z M 1.1875 -2.78125 L 3.796875 -2.78125 C 3.765625 -3.175781 3.664062 -3.472656 3.5 -3.671875 C 3.25 -3.972656 2.921875 -4.125 2.515625 -4.125 C 2.148438 -4.125 1.84375 -4 1.59375 -3.75 C 1.351562 -3.507812 1.21875 -3.1875 1.1875 -2.78125 Z M 1.1875 -2.78125 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-11\">\n<path style=\"stroke:none;\" d=\"M 0.578125 0 L 0.578125 -4.671875 L 1.296875 -4.671875 L 1.296875 -3.953125 C 1.472656 -4.285156 1.640625 -4.503906 1.796875 -4.609375 C 1.953125 -4.710938 2.125 -4.765625 2.3125 -4.765625 C 2.570312 -4.765625 2.84375 -4.679688 3.125 -4.515625 L 2.84375 -3.78125 C 2.65625 -3.894531 2.460938 -3.953125 2.265625 -3.953125 C 2.097656 -3.953125 1.941406 -3.898438 1.796875 -3.796875 C 1.660156 -3.691406 1.5625 -3.546875 1.5 -3.359375 C 1.414062 -3.078125 1.375 -2.769531 1.375 -2.4375 L 1.375 0 Z M 0.578125 0 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph2-12\">\n<path style=\"stroke:none;\" d=\"M 0.4375 -3.140625 C 0.4375 -4.203125 0.722656 -5.035156 1.296875 -5.640625 C 1.867188 -6.253906 2.609375 -6.5625 3.515625 -6.5625 C 4.109375 -6.5625 4.644531 -6.414062 5.125 -6.125 C 5.601562 -5.84375 5.96875 -5.445312 6.21875 -4.9375 C 6.46875 -4.425781 6.59375 -3.851562 6.59375 -3.21875 C 6.59375 -2.5625 6.460938 -1.972656 6.203125 -1.453125 C 5.941406 -0.941406 5.566406 -0.550781 5.078125 -0.28125 C 4.597656 -0.0195312 4.078125 0.109375 3.515625 0.109375 C 2.910156 0.109375 2.367188 -0.0351562 1.890625 -0.328125 C 1.410156 -0.617188 1.046875 -1.019531 0.796875 -1.53125 C 0.554688 -2.039062 0.4375 -2.578125 0.4375 -3.140625 Z M 1.3125 -3.125 C 1.3125 -2.34375 1.519531 -1.726562 1.9375 -1.28125 C 2.351562 -0.84375 2.878906 -0.625 3.515625 -0.625 C 4.148438 -0.625 4.675781 -0.847656 5.09375 -1.296875 C 5.507812 -1.742188 5.71875 -2.382812 5.71875 -3.21875 C 5.71875 -3.738281 5.628906 -4.191406 5.453125 -4.578125 C 5.273438 -4.972656 5.015625 -5.28125 4.671875 -5.5 C 4.328125 -5.71875 3.945312 -5.828125 3.53125 -5.828125 C 2.925781 -5.828125 2.40625 -5.617188 1.96875 -5.203125 C 1.53125 -4.785156 1.3125 -4.09375 1.3125 -3.125 Z M 1.3125 -3.125 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph3-0\">\n<path style=\"stroke:none;\" d=\"M 6.546875 -10.65625 L 1.625 -10.65625 L 1.625 -0.75 L 6.546875 -0.75 Z M 7.359375 -11.390625 L 7.359375 -0.015625 L 0.8125 -0.015625 L 0.8125 -11.390625 Z M 7.359375 -11.390625 \"/>\n</symbol>\n<symbol overflow=\"visible\" id=\"glyph3-1\">\n<path style=\"stroke:none;\" d=\"M 12.4375 -1.78125 L 12.4375 0 L 14.203125 0 L 14.203125 -1.78125 Z M 7.109375 -1.78125 L 7.109375 0 L 8.875 0 L 8.875 -1.78125 Z M 1.78125 -1.78125 L 1.78125 0 L 3.546875 0 L 3.546875 -1.78125 Z M 1.78125 -1.78125 \"/>\n</symbol>\n</g>\n</defs>\n<g id=\"surface1\">\n<path style=\"fill-rule:nonzero;fill:rgb(69.804382%,85.098267%,100%);fill-opacity:1;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 124.015625 139.234375 C 128.996094 144.214844 128.996094 152.285156 124.015625 157.265625 C 119.035156 162.246094 110.964844 162.246094 105.984375 157.265625 C 101.003906 152.285156 101.003906 144.214844 105.984375 139.234375 C 110.964844 134.253906 119.035156 134.253906 124.015625 139.234375 \" transform=\"matrix(1,0,0,1,-24,-74)\"/>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-1\" x=\"86.803467\" y=\"76.635498\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph1-1\" x=\"91.303467\" y=\"78.635498\"/>\n</g>\n<path style=\"fill-rule:nonzero;fill:rgb(69.804382%,85.098267%,100%);fill-opacity:1;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 176.890625 139.234375 C 181.871094 144.214844 181.871094 152.285156 176.890625 157.265625 C 171.910156 162.246094 163.839844 162.246094 158.859375 157.265625 C 153.878906 152.285156 153.878906 144.214844 158.859375 139.234375 C 163.839844 134.253906 171.910156 134.253906 176.890625 139.234375 \" transform=\"matrix(1,0,0,1,-24,-74)\"/>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-1\" x=\"139.678467\" y=\"76.635498\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph1-2\" x=\"144.178467\" y=\"78.635498\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-1\" x=\"8.98755\" y=\"76.352783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-2\" x=\"11.48775\" y=\"76.352783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-3\" x=\"16.49355\" y=\"76.352783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-4\" x=\"21.49935\" y=\"76.352783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-5\" x=\"26.50515\" y=\"76.352783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-6\" x=\"29.00535\" y=\"76.352783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-7\" x=\"31.50555\" y=\"76.352783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-8\" x=\"33.50535\" y=\"76.352783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-9\" x=\"38.51115\" y=\"76.352783\"/>\n  <use xlink:href=\"#glyph2-10\" x=\"43.01115\" y=\"76.352783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-11\" x=\"48.01695\" y=\"76.352783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-12\" x=\"5.4873\" y=\"16.102783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-4\" x=\"12.4875\" y=\"16.102783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-5\" x=\"17.4933\" y=\"16.102783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-3\" x=\"19.9935\" y=\"16.102783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-4\" x=\"24.9993\" y=\"16.102783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-5\" x=\"30.0051\" y=\"16.102783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-6\" x=\"32.5053\" y=\"16.102783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-7\" x=\"35.0055\" y=\"16.102783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-8\" x=\"37.0053\" y=\"16.102783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-9\" x=\"42.0111\" y=\"16.102783\"/>\n  <use xlink:href=\"#glyph2-10\" x=\"46.5111\" y=\"16.102783\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph2-11\" x=\"51.5169\" y=\"16.102783\"/>\n</g>\n<path style=\"fill-rule:nonzero;fill:rgb(69.804382%,85.098267%,100%);fill-opacity:1;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 282.640625 138.484375 C 287.621094 143.464844 287.621094 151.535156 282.640625 156.515625 C 277.660156 161.496094 269.589844 161.496094 264.609375 156.515625 C 259.628906 151.535156 259.628906 143.464844 264.609375 138.484375 C 269.589844 133.503906 277.660156 133.503906 282.640625 138.484375 \" transform=\"matrix(1,0,0,1,-24,-74)\"/>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-1\" x=\"245.428467\" y=\"75.885498\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph1-3\" x=\"249.928467\" y=\"77.885498\"/>\n</g>\n<path style=\"fill-rule:nonzero;fill:rgb(39.99939%,74.902344%,100%);fill-opacity:1;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 202.640625 78.234375 C 207.621094 83.214844 207.621094 91.285156 202.640625 96.265625 C 197.660156 101.246094 189.589844 101.246094 184.609375 96.265625 C 179.628906 91.285156 179.628906 83.214844 184.609375 78.234375 C 189.589844 73.253906 197.660156 73.253906 202.640625 78.234375 \" transform=\"matrix(1,0,0,1,-24,-74)\"/>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph0-2\" x=\"165.175781\" y=\"15.635498\"/>\n</g>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph1-1\" x=\"170.181152\" y=\"17.635498\"/>\n</g>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 125.074219 140.433594 L 178.890625 98.683594 \" transform=\"matrix(1,0,0,1,-24,-74)\"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 182.050781 96.230469 L 178.890625 98.683594 M 177.96875 97.496094 L 182.050781 96.230469 L 179.808594 99.867188 \" transform=\"matrix(1,0,0,1,-24,-74)\"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 172.835938 136.5 L 186.371094 104.433594 \" transform=\"matrix(1,0,0,1,-24,-74)\"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 187.925781 100.75 L 186.371094 104.433594 M 184.988281 103.851562 L 187.925781 100.75 L 187.753906 105.019531 \" transform=\"matrix(1,0,0,1,-24,-74)\"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 263.4375 139.828125 L 208.523438 98.472656 \" transform=\"matrix(1,0,0,1,-24,-74)\"/>\n<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 205.328125 96.0625 L 208.523438 98.472656 M 207.621094 99.667969 L 205.328125 96.0625 L 209.425781 97.273438 \" transform=\"matrix(1,0,0,1,-24,-74)\"/>\n<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n  <use xlink:href=\"#glyph3-1\" x=\"188.75\" y=\"74.776001\"/>\n</g>\n</g>\n</svg>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.layer1(x)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "net = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initialize Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using net, we need to initialize the model parameters, such as the weights and biases in the linear\n",
    "regression model. we specify that each weight parameter should be randomly sampled from a normal distribution with mean 0 and standard deviation 0.01.\n",
    "The bias parameter will be initialized to zero by default. Both weight and bias will be attached with\n",
    "gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.layer1.weight.data = torch.Tensor(np.random.normal(size=(1, 2), scale=0.01, loc=0))\n",
    "net.layer1.bias.data = torch.Tensor([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above looks straightforward but in reality something quite strange is happening here. We are\n",
    "initializing parameters for a network even though we haven’t yet told nn how many dimensions the input\n",
    "will have. It might be 2 as in our example or it might be 2,000, so we couldn’t just preallocate enough space\n",
    "to make it work.\n",
    "nn let’s us get away with this because behind the scenes, the initialization is deferred until the first time\n",
    "that we attempt to pass data through our network. Just be careful to remember that since the parameters\n",
    "have not been initialized yet we cannot yet manipulate them in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define the Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In nn, there are many loss modules that defines various loss functions and we will directly use its implementation of squared loss (MSELoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": "loss = torch.nn.MSELoss(reduction=\"sum\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surpisingly, we aren’t the first people to implement mini-batch stochastic gradient descent, and thus\n",
    "torch supports SGD alongside a number of variations on this algorithm through its Trainer class. When\n",
    "we instantiate the Trainer, we’ll specify the parameters to optimize over (obtainable from our net via net.parameters()), the optimization algortihm we wish to use (sgd), and a dictionary of hyper-parameters\n",
    "required by our optimization algorithm. SGD just requires that we set the value learning_rate, (here we\n",
    "set it to 0.03).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that expressing our model through torch requires comparatively few lines of code.\n",
    "We didn’t have to individually allocate parameters, define our loss function, or implement stochastic gradient descent. Once we start working with much more complex models, the benefits of relying on torch\n",
    "abstractions will grow considerably. But once we have all the basic pieces in place, the training loop itself strikingly similar to what we did when implementing everything from scratch.\n",
    "To refresh your memory: for some number of epochs, we’ll make a complete pass over the dataset\n",
    "(train_data), grabbing one mini-batch of inputs and corresponding ground-truth labels at a time. \n",
    "\n",
    "For\n",
    "each batch, we’ll go through the following ritual:\n",
    "\n",
    "• Generate predictions by calling net(X) and calculate the loss l (the forward pass).\n",
    "\n",
    "• Calculate gradients by calling l.backward() (the backward pass).\n",
    "\n",
    "• Update the model parameters by invoking our SGD optimizer (note that trainer already knows which parameters to optimize over, so we just need to pass in the batch size.\n",
    "\n",
    "For good measure, we compute the loss after each epoch and print it to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.10021035373210907\n",
      "epoch 2, loss 0.10903461277484894\n",
      "epoch 3, loss 0.11273252218961716\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X), y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l_epoch = loss(net(features), labels)\n",
    "    print('epoch {}, loss {}'.format(epoch + 1, l_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters we have learned and the actual model parameters are compared as below. We get\n",
    "the layer we need from the net and access its weight (weight) and bias (bias). The parameters we have\n",
    "learned and the actual parameters are very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in estimating w tensor([-0.0026,  0.0029], grad_fn=<ThSubBackward>)\n",
      "Error in estimating b tensor(0.0023, grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "w = list(net.parameters())[0][0]\n",
    "print('Error in estimating w', true_w.reshape(w.shape) - w)\n",
    "b = list(net.parameters())[1][0]\n",
    "print('Error in estimating b', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If we replace l = loss(output, y) with l = loss(output, y).mean(), we need to change trainer.\n",
    "step(batch_size) to trainer.step(1) accordingly. Why?\n",
    "2. Review the pytorch documentation to see what loss functions and initialization methods are provided\n",
    "in the modules nn.loss and init. Replace the loss by Huber’s loss.\n",
    "3. How do you access the gradient of Linear.weight?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
